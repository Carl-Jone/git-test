0000000000000000
bert源码阅读
-理清：
	模型输入
	模型输入转换成向量的过程
	模型结构
	模型输出
	分类任务，fine-tuning微调，调的是什么？
	其它任务的微调怎么做？
	中文的词向量和句向量如何获取？
	

### 使用训练好的bert去实现MRPC任务去理解上述的一些问题

1、MRPC任务说明
MRPC任务，是一个二分类任务，判断两个句子意思是否相同，相同则为1，不同则为0

2、数据格式说明
	train数据格式如下
		Quality	#1 ID	#2 ID	#1 String	#2 String
		1	702876	702977	Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .	Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .
		0	2108705	2108831	Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .	Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .
	test数据格式如下，注意到这是没有标签的数据，index只是索引
		index	#1 ID	#2 ID	#1 String	#2 String
		0	1089874	1089925	PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So .	Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So .
		1	3019446	3019327	The world 's two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected .	Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash .
		2	1945605	1945824	According to the federal Centers for Disease Control and Prevention ( news - web sites ) , there were 19 reported cases of measles in the United States in 2002 .	The Centers for Disease Control and Prevention said there were 19 reported cases of measles in the United States in 2002 .

3、运行代码说明
数据有了，预训练模型也能下载，bert源码也能下载（其中就包含了有关这个任务的接口），那么怎么去训练和测试这个任务呢？
（1）首先，要配置环境变量（或者在运行代码的时候直接使用绝对路径）
	BERT_BASE_DIR和GLUE_DIR/MRPC，前者是下载好了预训练模型的路径，后者是数据集路径
（2）然后，运行代码
	python run_classifier.py \
		--task_name=MRPC \
		--do_train=true \
		--do_eval=true \
		--data_dir=$GLUE_DIR/MRPC \
		--vocab_file=$BERT_BASE_DIR/vocab.txt \
		--bert_config_file=$BERT_BASE_DIR/bert_config.json \
		--init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
		--max_seq_length=128 \
		--train_batch_size=8 \
		--learning_rate=2e-5 \
		--num_train_epochs=3.0 \
		--output_dir=/tmp/mrpc_output/
	其中，接口就写在run_classifier.py 中，指定task_name=MRPC就能去执行这个任务，之后当我们要去做其他分类任务时，一般做法是复制一份MRPC类的代码，在其中微调即可。
	这里最常见的问题就是内存不够，通常我们的GPU只有8G作用的显存，因此对于小的模型(bert-base)，我们最多使用batchsize=8，而如果要使用bert-large，那么batchsize只能设置成1。
（3）最后，结果如下
	***** Eval results *****
	eval_accuracy = 0.845588
	eval_loss = 0.505248
	global_step = 343
	loss = 0.505248

4、代码解析
（1）阅读run_classifier.py
（1.1）首先，是各种配置，这些配置都可以在上述的run命令里面去指定，其中有一些是必须指定的，可以看到main入口中规定了：
			flags.mark_flag_as_required("data_dir")
			flags.mark_flag_as_required("task_name")
			flags.mark_flag_as_required("vocab_file")
			flags.mark_flag_as_required("bert_config_file")
			flags.mark_flag_as_required("output_dir")
（1.2）其次，是各种类，主要可以分成两大类
	（1.2.1）数据处理类
			InputExample
			PaddingInputExample 
			InputFeatures
			DataProcessor
	（1.2.2）下游任务类
			XnliProcessor
			MnliProcessor
			MrpcProcessor
			ColaProcessor
（1.3）最后，是各种方法
			convert_single_example
			file_based_convert_examples_to_features
			file_based_input_fn_builder
			_truncate_seq_pair
			create_model
			model_fn_builder
			input_fn_builder
			convert_examples_to_features
			
（2）阅读tokenization.py
（2.1）首先，其中具有三个类
			FullTokenizer
			BasicTokenizer
			WordpieceTokenizer
（2.2）然后，是各种方法
			validate_case_matches_checkpoint
			convert_to_unicode
			printable_text
			load_vocab
			convert_by_vocab
			convert_tokens_to_ids
			convert_ids_to_tokens
			whitespace_tokenize
			_is_whitespace
			_is_control
			_is_punctuation
			
（3）以MRPC任务为例，趟一遍MRPC任务的执行过程，观察main方法，其中核心的内容如下：

（3.1）首先，观察MRPC类，其中包含两种方法，
	一是根据当前数据文件(train、test和dev)去构造InputExample，结果对于每个文件可以获得一个InputExample对象列表，而InputExample包含4个属性，第一个属性guid可认为是当前样本的索引id，其它text_a，text_b和label分别对应句子1，句子2和标签。
	二是获取标签列表方法。
（3.2）然后，观察FullTokenizer类，
	(3.2.1)__init__接受词表作为参数，初始化得到4个属性，分别如下：
		vocab 是一个{word:index, ...} 形式的字典
		inv_vocab 是对vocab的一个反转，是一个{index:word, ...} 形式的字典
		basic_tokenizer是一个BasicTokenizer对象，有关BasicTokenizer类，其中最重要的方法是tokenize，其过程是给定输入text，大致过程如下：
			convert_to_unicode 对输入text进行检查，进行py2和py3的兼容性考虑
			_clean_text 遍历text的每一个字符，过滤掉输入text中的未知字符、不常见的控制字符，再用空格替换掉text中的换行、回车、制表符等空白符
			_tokenize_chinese_chars 遍历text的每一个字符，当碰到中文字符时，会在前后各加一个空格
			whitespace_tokenize 按空格切分输入text成为一个word列表，其中英文是word，中文是char
			_run_strip_accents  在遍历word列表之后，会根据do_lower_case将word转换成小写，然后执行该方法，目的是为了去除某些word头上的音符
			_run_split_on_punc 对于输入一个句子来说，该方法会按标点符号切分句子，返回列表；但是在此处传入的是word，没什么特别作用，还是按原样返回
			whitespace_tokenize 按空格切分输入text成为一个word列表，其中英文是word，中文是char
		所以，综上basic_tokenizer的tokenize方法的作用是给定输入字符串，在经过去除特殊字符、空白符、中文切字处理、去除音符等操作，返回字符串中的所有word（中文则返回字）组成的列表。
		wordpiece_tokenizer是一个WordpieceTokenizer对象，其中只有一个方法是tokenize，作用是将每个单词依据词典切成更小的单位，比如对于unaffable，会返回['u', '##na', '##ff', '##able']；但是，当遇到的是一个正确的单词，返回的结果也只有这个单词，因为找的是最长子串，比如network，会返回['network']。
	（3.2.2）tokenize方法，给定输入句子，会先通过basic_tokenizer的tokenize方法获取预处理之后的word列表，然后再经过wordpiece_tokenizer的tokenize方法对每个word进行更细粒度的切分，最终得到的是将一个句子，切分成word_piece形式组成的列表。
	（3.2.3）convert_tokens_to_ids方法，是将上述通过tokenize方法获得的word_piece，即tokens根据vocab转换成index的形式，也就是当前word_piece在vocab中的位置。
	（3.2.4）convert_ids_to_tokens方法，是可以通过inv_vocab将index转换成word_piece的方法。
（3.3）接着要了解的是file_based_convert_examples_to_features方法，其具有若干参数，如下：
	train_examples 是通过MRPC对象去调用get_train_examples方法获得，这也是文本文件转换成模型输入特征的第一步，先转换成InputExample对象列表；
	label_list 是通过MRPC对象去调用get_labels方法获得；
	max_seq_length 是超参数，定义模型的最大输入序列长度；
	tokenizer 对象，是FullTokenizer实例；
	output_file 是模型输出文件；

	关于该方法，一个基本的过程是，遍历train_examples，对每一个example，也就是每一个InputExample对象去执行convert_single_example方法，获取单个样本的特征，下面重点就在于convert_single_example方法是如何将InputExample转换成InputFeatures对象的：
	convert_single_example方法参数：ex_index（当前example的索引）, example, label_list, max_seq_length, tokenizer
	InputFeatures属性：input_ids， input_mask， segment_ids， label_id， is_real_example
	转换过程：
		(a) 从example中获取text_a和text_b，首先对text_a和text_b做tokenize获取word列表tokens_a和tokens_b；
		(b) 当存在tokens_b时，要考察tokens_a + tokens_b的长度是否大于max_seq_length-3，如果不大于则不对tokens_a和tokens_b做改变，如果大于，则通过pop让tokens_a和tokens_b尽可能长度一致，并且总长度为max_seq_length-3；当tokens_b不存在时，仅考虑tokens_a长度和max_seq_length-2做比较，大于则截断，小于则不变；
		(c) 构造segment_ids：分别去遍历tokens_a和tokens_b，构造出模型的输入tokens = [CLS]tokens_a[SEP]tokens_b[SEP]，并且在这过程中segment_ids会记录tokens_a序列每个token为0，tokens_b序列每个token为1。即0 len(tokens_a)*0 0 len(tokens_b)*1 1
		(d) 构造input_ids：通过tokenizer.convert_tokens_to_ids将tokens转换成vocab中对应的index；
		(e) 构造input_mask：[1] * len(input_ids);
		(f) 以0填充input_ids、input_mask和segment_ids为max_seq_length长度；
		(g) 构造label_id：直接通过example的label属性获取；
		(h) 拿到了InputFeatures对象所有的属性，构造出来该对象；
		
	之后把获得到的每一个InputFeatures对象写入到output_file中，后续训练模型的时候，这个文件将作为模型的输入参数之一；
	至此，模型输入从文本 -> InputExample对象 -> InputFeatures对象构建完毕！
（3.4）模型构建的过程：待续




（4）梳理上述的内容
（4.1）从file到模型输入

（4.2）模型结构

（4.3）从模型输入到模型输出






